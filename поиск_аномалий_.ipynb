{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi/0NuScMvu/yLQjcsW092",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammadsulton1/MIPT_Data_analys/blob/main/%D0%BF%D0%BE%D0%B8%D1%81%D0%BA_%D0%B0%D0%BD%D0%BE%D0%BC%D0%B0%D0%BB%D0%B8%D0%B9_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CRX-vOTahA9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import scipy.stats as sts\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('ggplot')  # стиль для графиков\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg1iwBR2aoJ7"
      },
      "source": [
        "Зачем вообще искать аномалии?\n",
        "Предобработка данных: убираем выбросы, чтобы не переобучиться\n",
        "Анализ выбросов: хотим найти, понять и обезвредить\n",
        "Поиск аномалий как самоцель (поиск мошенников, подозрительного поведения пользователей)\n",
        "Поиск аномалий может быть как конечной целью анализа и построения моделей, так и промежуточным этапом подготовки и очистки данных. В первом сценарии мы хотим научиться для каждого объекта выборки выносить вердикт, является ли он аномальным/нестандартным, а во-втором мы находим и убираем выбросы в данных, чтобы в дальнейшем получить более устойчивые модели.\n",
        "\n",
        "В определении из документации scikit-learn, здача поиска аномалий разделяется на два возможных типа:\n",
        "\n",
        "Outlier detection (поиск выбросов): в тренировочной выборке содержатся выбросы, которые определяются как наблюдения, лежащие далеко от остальных. Таким образом, алгоритмы для детектирования выбросов пытаются найти регионы, где сосредоточена основная масса тренировочных данных, игрорируя аномальные наблюдения.\n",
        "Novelty detection (поиск \"новизны\"): тренировочная выборка не загрязнена выбросами, и мы хотим научиться отвечать на вопрос \"является ли новое наблюдение выбросом\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ODp3f5apoJ"
      },
      "source": [
        "#Сложности при поиске аномалий¶\n",
        "На практике задача поиска аномалий зачастую не сводится к построению бинарного классификатора \"выброс/не выброс\". Реальные данные редко бывают размечены и мы вынуждены использовать методы обучения без учителя.\n",
        "\n",
        "Одновременно с этим возникает вопрос о построении надежной схемы проверки результатов, ведь если \"правильных ответов\" у нас нет, то и понять, насколько алгоритм справляется со своей задачей, уже сложнее. Здесь очень помогут экспертные оценки о проценте аномальных объектов, которые ожидаются в выборке, так как с ними можно будет сравнивать прогнозные значения и варьировать тем самым чувствительность алгоритмов.\n",
        "\n",
        "Если же поиск и очистка от аномалий - это шаг в подготовке данных для последующего моделирования, то можно использовать стандартные подходы из машинного обученияб, о которых речь пойдёт на будующих неделях нашей специализации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c42w2UJRatuR"
      },
      "source": [
        "df = pd.read_csv('memes_prepare.csv', sep='\\t')\n",
        "df.set_index('name', inplace=True)\n",
        "\n",
        "# возьмём только числовые колонки\n",
        "df = df[['views', 'photos', 'comments', 'days_from_creation',\n",
        "          'average_views', 'average_comments', 'tags_len' ]]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQaeFCqHawAF"
      },
      "source": [
        "\n",
        "Описание колонок:\n",
        "\n",
        "name - название мема\n",
        "\n",
        "views - число просмотров на сайте\n",
        "\n",
        "comments - число комментариев\n",
        "\n",
        "photos - число вкладышей с мемом\n",
        "\n",
        "days_from_creation - сколько дней прошло от появления мема\n",
        "\n",
        "average_views - среднее число просмотров за день\n",
        "\n",
        "average_comments - среднее число комментариев за день\n",
        "\n",
        "tags_len - длина тегов в описании в числе символов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8QOjhlMau6w"
      },
      "source": [
        "\n",
        "df.hist(figsize=(20, 8));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiR6sVyva2BU"
      },
      "source": [
        "Прологарифмируем все переменные с длинными хвостами, чтобы на визуализациях аномалий можно было их увидеть."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ7O4siJa3lZ"
      },
      "source": [
        "for var in ['views', 'photos', 'average_views',\n",
        "            'average_comments', 'comments']:\n",
        "    df[var] = df[var].apply(lambda w: np.log(w + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9DaK-z3a438"
      },
      "source": [
        "df.hist(figsize=(20, 8));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJATNMJya61A"
      },
      "source": [
        "Мы видим, что даже после логарифмирования часть хвостов остались тяжёлыми. Давайте попробуем найти все аномальные мемы, лежащие в этих хвостах хотябы по одному из признаков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGETuMr-a9dZ"
      },
      "source": [
        "\n",
        "#2. Методы на основе описательных статистик¶\n",
        "#2.1 Правило 3-х сигм\n",
        "Всё, что оказалось за пределами трёх сигм - аномалия."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qskbca6-a82V"
      },
      "source": [
        "def outlier_std(data, col, threshold=3):\n",
        "    \"\"\"\n",
        "        Вычисляет для каждой строки является ли она аномалией\n",
        "    \"\"\"\n",
        "\n",
        "    mean = data[col].mean()\n",
        "    std = data[col].std()\n",
        "\n",
        "    up_bound = mean + threshold * std\n",
        "    low_bound = mean - threshold * std\n",
        "\n",
        "    anomalies = pd.concat([data[col] > up_bound, data[col] < low_bound], axis=1).any(axis=1)\n",
        "    return anomalies, up_bound, low_bound"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdUgibuTbClo"
      },
      "source": [
        "\n",
        "a,l,r = outlier_std(df, df.columns)\n",
        "l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbpyvaRbbET6"
      },
      "source": [
        "def get_column_outliers(data, function=outlier_std, threshold=3):\n",
        "\n",
        "    # дополнительная колонка с отметкой является ли конкретное наблюдение аномалией\n",
        "    outliers = pd.Series(data=[False]*len(data), index=data.index, name='is_outlier')\n",
        "\n",
        "    # табличка для статистики по каждой колонке\n",
        "    comparison_table = {}\n",
        "\n",
        "    for column in data.columns:\n",
        "        anomalies, upper_bound, lower_bound = function(data, column, threshold=threshold)\n",
        "        comparison_table[column] = [upper_bound, lower_bound, sum(anomalies), 100*sum(anomalies)/len(anomalies)]\n",
        "        outliers.loc[anomalies[anomalies].index] = True\n",
        "\n",
        "    comparison_table = pd.DataFrame(comparison_table).T\n",
        "    comparison_table.columns=['upper_bound', 'lower_bound', 'anomalies_count', 'anomalies_percentage']\n",
        "    return comparison_table, outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-jMDSssbGAs"
      },
      "source": [
        "comparison_table, std_outliers = get_column_outliers(df)\n",
        "\n",
        "# статистика по каждой колонке и числу аномалий в ней\n",
        "comparison_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8LLihLTbHdF"
      },
      "source": [
        "# какие наблюдения являются аномалиями, а какие нет\n",
        "std_outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-hobFqMbIz5"
      },
      "source": [
        "def anomalies_report(outliers):\n",
        "    print(\"Total number of outliers: {}\\nPercentage of outliers:   {:.2f}%\".format(\n",
        "            sum(outliers), 100*sum(outliers)/len(outliers)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcnkY5klbKRZ"
      },
      "source": [
        "anomalies_report(std_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS7NWymabLox"
      },
      "source": [
        "labeled_df = df.copy()\n",
        "labeled_df['is_outlier'] = std_outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkaQ6eAlbNLp"
      },
      "source": [
        "sns.pairplot(data=labeled_df, vars=df.columns,\n",
        "             hue='is_outlier', hue_order=[1, 0],\n",
        "             markers=['x', 'o'],  palette='bright');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdeWAW1nbOy9"
      },
      "source": [
        "\n",
        "#2.2 Межквартильное отклонение¶\n",
        "Всё, что оказалось за пределами трёх межквартильных отклонений - аномалия."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhZUXSyNbREq"
      },
      "source": [
        "def outlier_iqr(data, col, threshold = 3):\n",
        "\n",
        "    # интерквантильный размах\n",
        "    IQR = data[col].quantile(0.75) - data[col].quantile(0.25)\n",
        "\n",
        "    # насколько размахов отступать\n",
        "    up_bound = data[col].quantile(0.75) + (IQR * threshold)\n",
        "    low_bound = data[col].quantile(0.25) - (IQR * threshold)\n",
        "\n",
        "    anomalies = pd.concat([data[col]>up_bound, data[col]<low_bound], axis=1).any(axis=1)\n",
        "    return anomalies, up_bound, low_bound"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt-Y1qj4bRHN"
      },
      "source": [
        "comparison_table, iqr_outliers = get_column_outliers(df, function=outlier_iqr)\n",
        "anomalies_report(iqr_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRz-UUuAbRJs"
      },
      "source": [
        "comparison_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul-3ESqrbRL-"
      },
      "source": [
        "labeled_df = df.copy()\n",
        "labeled_df['is_outlier'] = iqr_outliers\n",
        "\n",
        "sns.pairplot(data=labeled_df, vars = df.columns,\n",
        "             hue='is_outlier', hue_order=[1, 0],\n",
        "             markers=['x', 'o'],  palette='bright');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTtBGgLVbeqR"
      },
      "source": [
        "#3. Кластеризация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58oElGEnbgGh"
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "class DistanceOutliers(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Distance based outlier detector model\n",
        "\n",
        "    Fit method calculates centroid of training samples and\n",
        "    using `metric` get distances from centroid to train samples.\n",
        "    Having distances - we calculate `threshold` based on `percentile`.\n",
        "\n",
        "    Predict method uses `threshold` and `metric` to determine, whether\n",
        "    distance to sample from centroid is too large -> outlier.\n",
        "\n",
        "    -----------\n",
        "    Parameters:\n",
        "\n",
        "    - metric: string, default - euclidean\n",
        "        metric to use for distance calculation (see scipy.spatial.distance.cdist)\n",
        "\n",
        "    - percentile: float in range [0, 100]\n",
        "        hyperparameter which sets the threshold for anomalies\n",
        "    \"\"\"\n",
        "    def __init__(self, metric='euclidean', percentile=90):\n",
        "        self.metric = metric\n",
        "        self.percentile = percentile\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.centroid = np.mean(X, axis=0).values.reshape(-1, 1).T\n",
        "        distances_train = cdist(self.centroid, X, metric=self.metric).reshape(-1)\n",
        "        self.threshold = np.percentile(distances_train, self.percentile)\n",
        "\n",
        "    def predict(self, X):\n",
        "        distances = cdist(self.centroid, X, metric=self.metric).reshape(-1)\n",
        "        predictions = (distances > self.threshold).astype(int)\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq3x9T5WbgJB"
      },
      "source": [
        "scaler = RobustScaler()\n",
        "scaled_data = pd.DataFrame(\n",
        "    data=scaler.fit_transform(data_features),\n",
        "    columns=data_features.columns\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HOl292dbgL-"
      },
      "source": [
        "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
        "tsne = TSNE(perplexity=50, n_jobs=-1)\n",
        "tsne_transformed = tsne.fit_transform(scaled_data)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(tsne_transformed[:, 0], tsne_transformed[:, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TzBqDpMbgOE"
      },
      "source": [
        "euclidian_model = DistanceOutliers(metric='euclidean', percentile=90)\n",
        "euclidian_model.fit(scaled_data)\n",
        "euclidian_outliers = euclidian_model.predict(scaled_data)\n",
        "anomalies_report(euclidian_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE-js2epbqWT"
      },
      "source": [
        "labeled_data = data_features.copy()\n",
        "labeled_data['is_outlier'] = euclidian_outliers\n",
        "\n",
        "sns.pairplot(data=labeled_data, vars = other_features,\n",
        "             hue='is_outlier', hue_order=[1, 0],\n",
        "             markers=['x', 'o'],  palette='bright')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH7CpoekbsNA"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(tsne_transformed[:, 0], tsne_transformed[:, 1], c=euclidian_outliers);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89eV2WQPbsPh"
      },
      "source": [
        "citiblock_model = DistanceOutliers(metric='cityblock', percentile=90)\n",
        "citiblock_model.fit(scaled_data)\n",
        "cityblock_outliers = citiblock_model.predict(scaled_data)\n",
        "anomalies_report(cityblock_outliers)\n",
        "\n",
        "labeled_data = data_features.copy()\n",
        "labeled_data['is_outlier'] = cityblock_outliers\n",
        "\n",
        "sns.pairplot(data=labeled_data, vars = other_features,\n",
        "             hue='is_outlier', hue_order=[1, 0],\n",
        "             markers=['x', 'o'],  palette='bright');\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(tsne_transformed[:, 0], tsne_transformed[:, 1], c=cityblock_outliers);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QhtCYp7bzim"
      },
      "source": [
        "Алгоритм:\n",
        "\n",
        "Выбираем случайную точку и находим её соседей в заданной окрестности\n",
        "Если соседей меньше критического значения – называем выбросами\n",
        "Если нет – объединяем в «плотный» кластер и повторяем поиск соседей\n",
        "Если все плотные точки пройдены и помечены как посещенные – выбираем новую не посещенную точку и начинаем сначала\n",
        "Повторяем, пока все точки не будут посещены\n",
        "\n",
        "Преимущества:\n",
        "\n",
        "Density-based (плотностной/вероятностный) метод – умеет в сложные формы кластеров\n",
        "Поиск выбросов и аномалий в данных\n",
        "Недостатки:\n",
        "\n",
        "Довольно сложный в настройке – очень чувствителен к параметру ”плотности” epsilon\n",
        "Идея - аномалии должны сильно отличаться от основных данных и скорее всего попадут в \"шум\". Почему бы не увеличивать epsilon до тех пор, пока все \"плотные\" данные не окажутся в нескольких немногочисленных кластерах, а шума будет столько, сколько мы подозреваем должно быть аномалий"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL7F9M3Qb1lK"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# для начала считаем все наблюдения аномальными\n",
        "outlier_percentage = 1.\n",
        "\n",
        "num_clusters = []\n",
        "anomaly_percentage = []\n",
        "\n",
        "# берем маленький эпсилон и начинаем увеличивать\n",
        "eps = 0.05\n",
        "eps_history = [eps]\n",
        "while outlier_percentage>0.1:\n",
        "    model = DBSCAN(eps=eps).fit(scaled_data)\n",
        "    labels = model.labels_\n",
        "    num_clusters.append(len(np.unique(labels))-1)\n",
        "    labels = np.array([1 if label == -1 else 0 for label in labels])\n",
        "    # считаем текущий процент \"шума\"\n",
        "    outlier_percentage = sum(labels==1) / len(labels)\n",
        "    eps += 0.05\n",
        "    eps_history.append(eps)\n",
        "    anomaly_percentage.append(outlier_percentage)\n",
        "\n",
        "model = DBSCAN(eps)\n",
        "model.fit(scaled_data)\n",
        "density_outlier = np.array([1 if label == -1 else 0 for label in model.labels_])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9hXDztwb3MC"
      },
      "source": [
        "anomalies_report(density_outlier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eMXBGhnbsSB"
      },
      "source": [
        "iterations = eps_history[:-1]\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('epsilon')\n",
        "ax1.set_ylabel('number of clusters', color=color)\n",
        "ax1.plot(iterations, num_clusters, color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('anomaly percentage', color=color)  # we already handled the x-label with ax1\n",
        "ax2.plot(iterations, anomaly_percentage, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCOZBsOkb5Ju"
      },
      "source": [
        "labeled_data = data_features.copy()\n",
        "labeled_data['is_outlier'] = density_outlier\n",
        "\n",
        "sns.pairplot(data=labeled_data, vars = other_features,\n",
        "             hue='is_outlier', hue_order=[1, 0],\n",
        "             markers=['x', 'o'],  palette='bright');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEtyW8e4b8_f"
      },
      "source": [
        "\n",
        "Коротко о Support Vector Machine\n",
        "Если совсем вкратце, SVM - базовая линейная модель. Основная идея алгоритма (в случае с классификацией) - разделить классы гиперплоскостью так, чтобы максимизировать расстояние (зазор) между ними. Изначально алгоритм был способен работать только с линейно разделимыми классами, однако в 90-е годы прошлого века метод стал особенно популярен из-за внедрения \"Kernel Trick\" (1992), позволившего эффективно работать с линейно неразделимыми данными.\n",
        "\n",
        "\n",
        "\n",
        "Kernel Trick\n",
        "Ядро (kernel) - это функция, которая способна преобразовать признаковое пространство (в том числе нелинейно), без непосредственного преобразования признаков.\n",
        "\n",
        "Крайне эффективна в плане вычисления и потенциально позволяет получать бесконечноразмерные признаковые пространства.\n",
        "\n",
        "Идея заключается в том, что классы, линейно неразделимые в текущем признаковом пространстве, могут стать разделимыми в пространствах более высокой размерности:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy0m0mh3cAzr"
      },
      "source": [
        "One Class SVM\n",
        "http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/\n",
        "\n",
        "One Class SVM - это одна из форм классического алгоритма, однако, как следует из названия, для его обучения нам достаточно иметь всего один класс, пусть даже и немного \"зашумленный\", при этом мы хотим научиться для каждого нового наблюдения принимать решение, является ли оно аномальным или нет.\n",
        "\n",
        "Общая идея: преобразовать признаковое пространство и провести разделяющую гиперплоскость так, чтобы наблюдения лежали как можно дальше от начала координат:\n",
        "\n",
        "\n",
        "\n",
        "В результате мы получаем границу, по одну сторону которой лежат максимально \"плотные\" и похожие друг на друга наблюдения из нашей тренировочной выборки, а по другую будут находится аномальные значения, не похожие на все остальные. Процент таких аномальных наблюдений, которые модель будет пытаться отделить от основной части выборки, мы снова задаём в самом начале обучения при помощи параметра nu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOx9QPSPb5MP"
      },
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "one_class_svm = OneClassSVM(nu=0.1, gamma='auto')\n",
        "one_class_svm.fit(scaled_data)\n",
        "svm_outliers = one_class_svm.predict(scaled_data)\n",
        "svm_outliers = np.array([1 if label == -1 else 0 for label in svm_outliers])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmNSzv3gb5PN"
      },
      "source": [
        "anomalies_report(svm_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abe03n48b5RQ"
      },
      "source": [
        "labeled_data = data_features.copy()\n",
        "labeled_data['is_outlier'] = svm_outliers\n",
        "\n",
        "sns.pairplot(data=labeled_data, vars = other_features,\n",
        "             hue='is_outlier', hue_order=[1, 0],\n",
        "             markers=['x', 'o'],  palette='bright');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tgf9dgPcH_C"
      },
      "source": [
        "\n",
        "Плюсы и минусы\n",
        "+ Благодаря kernel trick, модель способна проводить нелинейные разделяющие границы\n",
        "\n",
        "+ Особенно удобно использовать, когда в данных недостаточно \"плохих\" наблюдений, чтобы использовать стандартный подход обучения с учителем - бинарную классификацию\n",
        "\n",
        "- Может очень сильно переобучиться и выдавать большое количество ложно отрицательных результатов, если разделяющий зазор слишком мал\n",
        "\n",
        "Идея - давайте посмотрим, насколько легко можно \"изолировать\" наблюдение от всех остальных. Если слишком легко, наверное она лежит далеко и является выбросом. Если очень тяжело - скорее всего она похожа на кучу других точек и выбросом не является.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Алгоритм:\n",
        "\n",
        "Select the point to isolate.\n",
        "For each feature, set the range to isolate between the minimum and the maximum.\n",
        "Choose a feature randomly.\n",
        "Pick a value that’s in the range, again randomly:\n",
        "If the chosen value keeps the point above, switch the minimum of the range of the feature to the value.\n",
        "If the chosen value keeps the point below, switch the maximum of the range of the feature to the value.\n",
        "Repeat steps 3 & 4 until the point is isolated. That is, until the point is the only one which is inside the range for all features.\n",
        "Count how many times you’ve had to repeat steps 3 & 4. We call this quantity the isolation number.\n",
        "https://quantdare.com/isolation-forest-algorithm/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExGIhVIhcJ0t"
      },
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.1,\n",
        "                                   max_features=1.0, bootstrap=True, behaviour=\"new\")\n",
        "isolation_forest.fit(scaled_data)\n",
        "\n",
        "isolation_outliers = isolation_forest.predict(scaled_data)\n",
        "isolation_outliers = np.array([1 if label == -1 else 0 for label in isolation_outliers])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK5xxsIsb5Te"
      },
      "source": [
        "anomalies_report(isolation_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsGQYcrNb5WU"
      },
      "source": [
        "labeled_data = data_features.copy()\n",
        "labeled_data['is_outlier'] = isolation_outliers\n",
        "\n",
        "sns.pairplot(data=labeled_data, vars = other_features,\n",
        "             hue='is_outlier', hue_order=[1, 0],\n",
        "             markers=['x', 'o'],  palette='bright')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_v_qpXzcOR2"
      },
      "source": [
        "\n",
        "Плюсы и минусы\n",
        "+ Снова нелинейные разделяющие границы, интуитивно понятные принципы работы\n",
        "\n",
        "+ Снова удобно использовать, когда не можем в бинарную классификацию\n",
        "\n",
        "- Довольно сложная настройка параметров, которую тяжело проводить, не имея хотя бы минимальной разметки или предположения о количестве \"грязных\" наблюдений - параметр contamination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvBSWbZ9cP4F"
      },
      "source": [
        "##Финальное сравнение¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8WI_-ZTcRMF"
      },
      "source": [
        "summary = np.concatenate((\n",
        "    [std_outliers],\n",
        "    [iqr_outliers],\n",
        "    [euclidian_outliers],\n",
        "    [cityblock_outliers],\n",
        "    [density_outlier],\n",
        "    [svm_outliers],\n",
        "    [isolation_outliers]\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGdeHUPTcUg9"
      },
      "source": [
        "\n",
        "summary = pd.DataFrame(\n",
        "    summary.T,\n",
        "    columns=['std', 'iqr', 'euclid', 'cityblock', 'dbscan', 'svm', 'isolation']\n",
        ")\n",
        "summary.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTRzNT2hcUsC"
      },
      "source": [
        "summary.sum(axis=1).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mke8L2sScax4"
      },
      "source": [
        "outlier_score = summary.mean(axis=1)\n",
        "plt.hist(outlier_score, alpha=0.6);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBwp8Rztce0R"
      },
      "source": [
        "##All combined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCx74A05cUz5"
      },
      "source": [
        "simple_score = outlier_score.apply(lambda x: 0 if x < 0.4 else 0.5 if x < 0.8 else 1)\n",
        "\n",
        "labeled_data = data_features.copy()\n",
        "labeled_data['outlier_score'] = simple_score\n",
        "\n",
        "custom_palette = {0:'g', 0.5:'b', 1.0:'r'}\n",
        "\n",
        "sns.pairplot(data=labeled_data, vars=other_features,\n",
        "             hue='outlier_score',\n",
        "             hue_order=[1, 0.5, 0],\n",
        "             palette=custom_palette\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk6g0YQGckA6"
      },
      "source": [
        "##Just model based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYRp8vlFclaq"
      },
      "source": [
        "\n",
        "outliers_score_model_based = summary[['dbscan', 'svm', 'isolation']].sum(axis=1)\n",
        "plt.hist(outliers_score_model_based, alpha=0.6);\n",
        "outliers_score_model_based.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rn2Hn2ucldJ"
      },
      "source": [
        "labeled_data = data_features.copy()\n",
        "labeled_data['outlier_score'] = outliers_score_model_based\n",
        "\n",
        "custom_palette = {0:'g', 1:'b', 2:'y', 3:'r'}\n",
        "\n",
        "sns.pairplot(data=labeled_data, vars=other_features,\n",
        "             hue='outlier_score',\n",
        "             hue_order=[3, 2, 1, 0],\n",
        "             palette=custom_palette\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C_tBMrsclf9"
      },
      "source": [
        "sns.pairplot(data=labeled_data[labeled_data.outlier_score!=3],\n",
        "             vars=other_features,\n",
        "             hue='outlier_score',\n",
        "             hue_order=[3, 2, 1, 0],\n",
        "             palette=custom_palette\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkibZhcaclim"
      },
      "source": [
        "sns.pairplot(data=labeled_data[labeled_data.outlier_score.isin([0, 1])],\n",
        "             vars=other_features,\n",
        "             hue='outlier_score',\n",
        "             hue_order=[3, 2, 1, 0],\n",
        "             palette=custom_palette\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfWAixOcuLC"
      },
      "source": [
        "sns.pairplot(data=labeled_data[labeled_data.outlier_score.isin([0])],\n",
        "             vars=other_features,\n",
        "             hue='outlier_score',\n",
        "             hue_order=[3, 2, 1, 0],\n",
        "             palette=custom_palette\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4l4vHCmcw9z"
      },
      "source": [
        "\n",
        "Дополнительные материалы\n",
        "\n",
        "Поиск аномалий во временных рядах https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3\n",
        "Плотностные методы для поиска аномалий https://towardsdatascience.com/density-based-algorithm-for-outlier-detection-8f278d2f7983\n",
        "Верхнеуровневая библиотека с кучей методов https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/\n",
        "Еще раз про классику https://blog.floydhub.com/introduction-to-anomaly-detection-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsVofHX4cuNh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-tihEcwcuQH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4KQlRqCcuSo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4BMwk_UcuVP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjT8c8h0cuXk"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njzX4N2qcuaC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}